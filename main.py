#!/usr/bin/env python3
# main.py - SEO Analyzer Modularizado - VERS√ÉO SIMPLIFICADA

"""
üè∑Ô∏è SEO ANALYZER ULTRA CORRIGIDO - VERS√ÉO SIMPLES

Analisador de SEO modularizado com todas as corre√ß√µes implementadas:
‚úÖ Hierarquia de headings corrigida (ignora headings problem√°ticos)
‚úÖ Detec√ß√£o expandida de headings ocultos (cores invis√≠veis)
‚úÖ Consolida√ß√£o em aba √∫nica para headings problem√°ticos  
‚úÖ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)
‚úÖ Sequ√™ncias separadas (completa vs. v√°lida)

üöÄ USO SIMPLES:
    python main.py                    # Usa URL padr√£o
    python main.py --url https://exemplo.com
    python main.py --max-urls 500     # An√°lise r√°pida
"""

import argparse
import sys
from urllib.parse import urlparse

# Imports dos m√≥dulos modularizados
from config.settings import get_config, DEFAULT_URL, MAX_URLS_DEFAULT, MAX_THREADS_DEFAULT
from core.crawler import create_crawler
from analyzers.metatags_analyzer import create_metatags_analyzer
from reports.excel_generator import create_report_generator
from utils.constants import (
    MSG_CRAWLER_START, MSG_ANALYSIS_START, MSG_ANALYSIS_COMPLETE,
    MSG_CORRECTIONS_IMPLEMENTED, MSG_IMPROVEMENTS, MSG_NEW_CONSOLIDATED_TAB
)


def parse_arguments():
    """üìã Parse argumentos da linha de comando (TODOS OPCIONAIS)"""
    parser = argparse.ArgumentParser(
        description='üè∑Ô∏è SEO Analyzer Ultra Corrigido - An√°lise completa de metatags',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
üöÄ USO SIMPLIFICADO:
  python main.py                           # An√°lise completa da URL padr√£o
  python main.py --url https://seusite.com # An√°lise de dom√≠nio espec√≠fico
  python main.py --max-urls 100            # An√°lise r√°pida (100 URLs)
  python main.py --threads 10              # Usar 10 threads

‚úÖ TODAS AS CORRE√á√ïES IMPLEMENTADAS:
  ‚Ä¢ Headings vazios/ocultos ignorados na an√°lise hier√°rquica
  ‚Ä¢ Detec√ß√£o expandida de headings ocultos (cores invis√≠veis)
  ‚Ä¢ Consolida√ß√£o em aba √∫nica "Headings_Problematicos"
  ‚Ä¢ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)
  ‚Ä¢ Sequ√™ncias separadas (Completa vs. V√°lida)

üéØ URL PADR√ÉO: {default_url}
üìä CONFIGURA√á√ÉO PADR√ÉO: {max_urls} URLs, {threads} threads, profundidade m√°xima
        """.format(
            default_url=DEFAULT_URL,
            max_urls=MAX_URLS_DEFAULT,
            threads=MAX_THREADS_DEFAULT
        )
    )
    
    # TODOS OS ARGUMENTOS S√ÉO OPCIONAIS
    parser.add_argument(
        '--url',
        type=str,
        default=DEFAULT_URL,  # üî• USA URL PADR√ÉO
        help=f'URL inicial para an√°lise (padr√£o: {DEFAULT_URL})'
    )
    
    parser.add_argument(
        '--max-urls',
        type=int,
        default=MAX_URLS_DEFAULT,
        help=f'N√∫mero m√°ximo de URLs para analisar (padr√£o: {MAX_URLS_DEFAULT})'
    )
    
    parser.add_argument(
        '--max-depth',
        type=int,
        default=10,
        help='Profundidade m√°xima de crawling (padr√£o: 10)'
    )
    
    parser.add_argument(
        '--threads',
        type=int,
        default=MAX_THREADS_DEFAULT,
        help=f'N√∫mero de threads para crawling (padr√£o: {MAX_THREADS_DEFAULT})'
    )
    
    parser.add_argument(
        '--crawler',
        choices=['default', 'smart', 'batch'],
        default='smart',
        help='Tipo de crawler a usar (padr√£o: smart)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='output',
        help='Pasta de sa√≠da para relat√≥rios (padr√£o: output)'
    )
    
    parser.add_argument(
        '--filename',
        type=str,
        default='METATAGS_ULTRA',
        help='Prefixo do nome do arquivo de relat√≥rio (padr√£o: METATAGS_ULTRA)'
    )
    
    parser.add_argument(
        '--quick',
        action='store_true',
        help='An√°lise r√°pida (100 URLs, 5 threads)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Modo verboso com mais detalhes'
    )
    
    return parser.parse_args()


def validate_arguments(args):
    """‚úÖ Valida argumentos fornecidos"""
    errors = []
    
    # Valida URL
    try:
        parsed_url = urlparse(args.url)
        if not parsed_url.scheme or not parsed_url.netloc:
            errors.append(f"‚ùå URL inv√°lida: {args.url}")
    except Exception:
        errors.append(f"‚ùå Erro parsing URL: {args.url}")
    
    # Valida n√∫meros
    if args.max_urls <= 0:
        errors.append("‚ùå max-urls deve ser maior que 0")
    
    if args.max_depth <= 0:
        errors.append("‚ùå max-depth deve ser maior que 0")
    
    if args.threads <= 0 or args.threads > 50:
        errors.append("‚ùå threads deve estar entre 1 e 50")
    
    return errors


def apply_quick_mode(args):
    """‚ö° Aplica configura√ß√µes de modo r√°pido"""
    if args.quick:
        args.max_urls = min(args.max_urls, 100)
        args.threads = min(args.threads, 5)
        args.max_depth = min(args.max_depth, 5)
        print("‚ö° MODO R√ÅPIDO ativado: 100 URLs m√°x, 5 threads, profundidade 5")


def create_config_from_args(args):
    """üîß Cria configura√ß√£o baseada nos argumentos"""
    config = get_config()
    
    # Atualiza configura√ß√µes do crawler
    config['crawler'].update({
        'max_urls': args.max_urls,
        'max_depth': args.max_depth,
        'max_threads': args.threads,
        'timeout': 15
    })
    
    # Configura√ß√µes de sa√≠da
    config['output'].update({
        'folder': args.output,
        'use_emoji_names': True,
        'filename_prefix': args.filename
    })
    
    # Configura√ß√µes espec√≠ficas do crawler
    if args.crawler == 'smart':
        config['priority_patterns'] = [
            '/produto/', '/product/', '/categoria/', '/category/',
            '/servico/', '/service/', '/sobre/', '/about/',
            '/contato/', '/contact/', '/planos/', '/plan'
        ]
    elif args.crawler == 'batch':
        config['batch_size'] = 50
    
    return config


def print_startup_info(args, config):
    """üì¢ Exibe informa√ß√µes de inicializa√ß√£o"""
    domain = urlparse(args.url).netloc
    
    print("=" * 80)
    print("üè∑Ô∏è  SEO ANALYZER ULTRA CORRIGIDO - AN√ÅLISE COMPLETA")
    print("=" * 80)
    
    print("üî• CONFIGURA√á√ÉO OTIMIZADA:")
    print("   ‚úÖ Busca TODAS as p√°ginas poss√≠veis do dom√≠nio")
    print("   ‚úÖ Crawler inteligente com prioriza√ß√£o autom√°tica")
    print("   ‚úÖ Multi-threading para alta velocidade")
    print("   ‚úÖ Filtros autom√°ticos para URLs relevantes")
    
    print(MSG_CORRECTIONS_IMPLEMENTED)
    print(MSG_IMPROVEMENTS)
    print(MSG_NEW_CONSOLIDATED_TAB)
    
    print(f"\nüìä CONFIGURA√á√ïES DE AN√ÅLISE:")
    print(f"   üéØ URL inicial: {args.url}")
    print(f"   üìà Max URLs: {args.max_urls:,}")
    print(f"   üìè Max profundidade: {args.max_depth}")
    print(f"   ‚ö° Threads: {args.threads}")
    print(f"   üï∑Ô∏è Tipo de crawler: {args.crawler}")
    print(f"   üìÅ Pasta de sa√≠da: {args.output}")
    
    print(f"\nüöÄ Iniciando an√°lise do dom√≠nio: {domain}")
    print("=" * 80)


def main():
    """üöÄ Fun√ß√£o principal"""
    try:
        # 1. Parse e valida√ß√£o de argumentos (TODOS OPCIONAIS)
        args = parse_arguments()
        
        # 2. Aplica modo r√°pido se solicitado
        apply_quick_mode(args)
        
        # 3. Valida√ß√£o
        validation_errors = validate_arguments(args)
        if validation_errors:
            print("‚ùå ERROS DE VALIDA√á√ÉO:")
            for error in validation_errors:
                print(f"   {error}")
            sys.exit(1)
        
        # 4. Cria configura√ß√£o
        config = create_config_from_args(args)
        
        # 5. Exibe informa√ß√µes de inicializa√ß√£o
        print_startup_info(args, config)
        
        # 6. Cria componentes modulares
        print("üîß Inicializando componentes...")
        
        crawler = create_crawler(args.crawler, config)
        print(f"   ‚úÖ Crawler '{args.crawler}' criado")
        
        analyzer = create_metatags_analyzer('default', config)
        print(f"   ‚úÖ Analisador de metatags criado")
        
        report_generator = create_report_generator('default', config['output'])
        print(f"   ‚úÖ Gerador de relat√≥rios criado")
        
        # 7. Executa crawling e an√°lise
        print("\nüï∑Ô∏è FASE 1: CRAWLING E AN√ÅLISE")
        print(MSG_CRAWLER_START.format(domain=urlparse(args.url).netloc))
        
        results = crawler.crawl(
            start_url=args.url,
            max_urls=args.max_urls,
            analyzers=[analyzer]
        )
        
        if not results:
            print("‚ùå Nenhum resultado obtido do crawling!")
            sys.exit(1)
        
        print(MSG_ANALYSIS_COMPLETE.format(total_urls=len(results)))
        
        # 8. Gera relat√≥rio
        print("\nüìä FASE 2: GERA√á√ÉO DE RELAT√ìRIOS")
        
        filepath, df_principal = report_generator.generate_complete_report(
            results=results,
            crawlers_data=analyzer,
            filename_prefix=args.filename
        )
        
        if not filepath:
            print("‚ùå Erro na gera√ß√£o do relat√≥rio!")
            sys.exit(1)
        
        # 9. Exibe estat√≠sticas finais
        print("\nüìà FASE 3: ESTAT√çSTICAS FINAIS")
        print("=" * 80)
        
        # Estat√≠sticas do crawler
        crawler_stats = crawler.get_stats()
        print("üï∑Ô∏è ESTAT√çSTICAS DO CRAWLER:")
        print(f"   URLs encontradas: {crawler_stats['crawling']['urls_found']}")
        print(f"   URLs processadas: {crawler_stats['crawling']['urls_processed']}")
        print(f"   Taxa de sucesso: {crawler_stats['summary']['success_rate']:.1f}%")
        print(f"   Tempo total: {crawler_stats['summary']['total_crawling_time']:.2f}s")
        print(f"   URLs/segundo: {crawler_stats['summary']['urls_per_second']:.2f}")
        
        # Estat√≠sticas do analisador
        analyzer_stats = analyzer.get_stats()
        print(f"\nüè∑Ô∏è ESTAT√çSTICAS DO ANALISADOR:")
        print(f"   URLs analisadas: {analyzer_stats['processing']['urls_processadas']}")
        print(f"   Titles duplicados √∫nicos: {analyzer_stats['duplicates']['total_duplicate_titles']}")
        print(f"   Descriptions duplicadas √∫nicas: {analyzer_stats['duplicates']['total_duplicate_descriptions']}")
        
        # Estat√≠sticas do relat√≥rio
        if not df_principal.empty:
            print(f"\nüìä ESTAT√çSTICAS DO RELAT√ìRIO:")
            print(f"   P√°ginas no relat√≥rio: {len(df_principal)}")
            
            # Score m√©dio
            score_medio = df_principal['Metatags_Score'].mean()
            print(f"   Score m√©dio: {score_medio:.1f}/100")
            
            # Problemas cr√≠ticos
            criticos = len(df_principal[df_principal['Critical_Issues'] != ''])
            print(f"   URLs com problemas cr√≠ticos: {criticos}")
            
            # üÜï Headings problem√°ticos (NOVA M√âTRICA)
            headings_problematicos = len(df_principal[df_principal['Headings_Problematicos_Total'] > 0])
            print(f"   üÜï URLs com headings problem√°ticos: {headings_problematicos}")
            
            # Top 3 problemas
            if criticos > 0:
                top_issues = []
                
                # Title ausente
                title_ausente = len(df_principal[df_principal['Title_Status'] == 'Ausente'])
                if title_ausente > 0:
                    top_issues.append(f"Titles ausentes ({title_ausente})")
                
                # Description ausente
                desc_ausente = len(df_principal[df_principal['Description_Status'] == 'Ausente'])
                if desc_ausente > 0:
                    top_issues.append(f"Descriptions ausentes ({desc_ausente})")
                
                # H1 ausente
                h1_ausente = len(df_principal[df_principal['H1_Ausente'] == 'SIM'])
                if h1_ausente > 0:
                    top_issues.append(f"H1s ausentes ({h1_ausente})")
                
                if top_issues:
                    print(f"   Top problemas: {', '.join(top_issues[:3])}")
        
        # 10. Informa√ß√µes finais
        print("\nüéØ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
        print("=" * 80)
        print(f"üìÅ Relat√≥rio gerado: {filepath}")
        print("\nüí° PRINCIPAIS CORRE√á√ïES IMPLEMENTADAS:")
        print("   ‚úÖ Headings vazios/ocultos IGNORADOS na an√°lise hier√°rquica")
        print("   ‚úÖ Detec√ß√£o EXPANDIDA de headings ocultos (cores invis√≠veis)")
        print("   ‚úÖ Consolida√ß√£o em aba √∫nica 'Headings_Problematicos'")
        print("   ‚úÖ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)")
        print("   ‚úÖ Sequ√™ncias separadas (Completa vs. V√°lida)")
        
        print(f"\nüî• Abra o arquivo Excel para ver todas as 8 abas com as corre√ß√µes!")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è An√°lise interrompida pelo usu√°rio")
        sys.exit(0)
        
    except Exception as e:
        print(f"\n‚ùå ERRO INESPERADO: {str(e)}")
        if hasattr(args, 'verbose') and args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def quick_analysis(url=None, max_urls=1000):
    """‚ö° Fun√ß√£o para an√°lise r√°pida (uso program√°tico)"""
    if url is None:
        url = DEFAULT_URL
    
    config = get_config()
    config['crawler']['max_urls'] = max_urls
    config['crawler']['max_depth'] = 5
    config['crawler']['max_threads'] = 10
    
    # Componentes com configura√ß√£o otimizada
    crawler = create_crawler('smart', config)
    analyzer = create_metatags_analyzer('default', config)
    report_generator = create_report_generator('default', config['output'])
    
    # Execu√ß√£o
    results = crawler.crawl(url, max_urls, [analyzer])
    
    if results:
        filepath, df = report_generator.generate_complete_report(
            results, analyzer, "QUICK_ANALYSIS"
        )
        return filepath, df, analyzer.get_stats()
    
    return None, None, None


if __name__ == "__main__":
    main()