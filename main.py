#!/usr/bin/env python3
# main.py - SEO Analyzer Modularizado - VERS√ÉO CORRIGIDA

"""
üè∑Ô∏è SEO ANALYZER ULTRA CORRIGIDO

Analisador de SEO modularizado com todas as corre√ß√µes implementadas:
‚úÖ Hierarquia de headings corrigida (ignora headings problem√°ticos)
‚úÖ Detec√ß√£o expandida de headings ocultos (cores invis√≠veis)
‚úÖ Consolida√ß√£o em aba √∫nica para headings problem√°ticos  
‚úÖ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)
‚úÖ Sequ√™ncias separadas (completa vs. v√°lida)

Uso:
    python main.py --url https://example.com --max-urls 100 --threads 10
"""

import argparse
import sys
from urllib.parse import urlparse

# Imports dos m√≥dulos modularizados
from config.settings import get_config, DEFAULT_URL, MAX_URLS_DEFAULT, MAX_THREADS_DEFAULT
from core.crawler import create_crawler
from analyzers.metatags_analyzer import create_metatags_analyzer
from reports.excel_generator import create_report_generator
from utils.constants import (
    MSG_CRAWLER_START, MSG_ANALYSIS_START, MSG_ANALYSIS_COMPLETE,
    MSG_CORRECTIONS_IMPLEMENTED, MSG_IMPROVEMENTS, MSG_NEW_CONSOLIDATED_TAB
)


def parse_arguments():
    """üìã Parse argumentos da linha de comando"""
    parser = argparse.ArgumentParser(
        description='üè∑Ô∏è SEO Analyzer Ultra Corrigido - An√°lise completa de metatags',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python main.py --url https://example.com
  python main.py --url https://example.com --max-urls 50 --threads 5
  python main.py --url https://example.com --crawler smart --output custom_output/
  python main.py --url https://example.com --analyzer batch --batch-size 200

Tipos de crawler:
  default - Crawler padr√£o
  smart   - Crawler com prioriza√ß√£o inteligente  
  batch   - Crawler otimizado para lotes grandes

Corre√ß√µes implementadas:
  ‚úÖ Headings vazios/ocultos ignorados na an√°lise hier√°rquica
  ‚úÖ Detec√ß√£o expandida de headings ocultos (cores invis√≠veis)
  ‚úÖ Consolida√ß√£o em aba √∫nica "Headings_Problematicos"
  ‚úÖ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)
        """
    )
    
    parser.add_argument(
        '--url',
        type=str,
        default=DEFAULT_URL,
        help=f'URL inicial para an√°lise (padr√£o: {DEFAULT_URL})'
    )
    
    parser.add_argument(
        '--max-urls',
        type=int,
        default=MAX_URLS_DEFAULT,
        help=f'N√∫mero m√°ximo de URLs para analisar (padr√£o: {MAX_URLS_DEFAULT})'
    )
    
    parser.add_argument(
        '--max-depth',
        type=int,
        default=5,
        help='Profundidade m√°xima de crawling (padr√£o: 5)'
    )
    
    parser.add_argument(
        '--threads',
        type=int,
        default=MAX_THREADS_DEFAULT,
        help=f'N√∫mero de threads para crawling (padr√£o: {MAX_THREADS_DEFAULT})'
    )
    
    parser.add_argument(
        '--crawler',
        choices=['default', 'smart', 'batch'],
        default='default',
        help='Tipo de crawler a usar (padr√£o: default)'
    )
    
    parser.add_argument(
        '--analyzer',
        choices=['default', 'batch'],
        default='default',
        help='Tipo de analisador a usar (padr√£o: default)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=100,
        help='Tamanho do lote para processamento (padr√£o: 100)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='output',
        help='Pasta de sa√≠da para relat√≥rios (padr√£o: output)'
    )
    
    parser.add_argument(
        '--emoji-names',
        action='store_true',
        help='Usar nomes com emoji nas abas do Excel'
    )
    
    parser.add_argument(
        '--filename',
        type=str,
        default='METATAGS_ULTRA',
        help='Prefixo do nome do arquivo de relat√≥rio (padr√£o: METATAGS_ULTRA)'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=15,
        help='Timeout para requisi√ß√µes em segundos (padr√£o: 15)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Modo verboso com mais detalhes'
    )
    
    return parser.parse_args()


def validate_arguments(args):
    """‚úÖ Valida argumentos fornecidos"""
    errors = []
    
    # Valida URL
    try:
        parsed_url = urlparse(args.url)
        if not parsed_url.scheme or not parsed_url.netloc:
            errors.append(f"‚ùå URL inv√°lida: {args.url}")
    except Exception:
        errors.append(f"‚ùå Erro parsing URL: {args.url}")
    
    # Valida n√∫meros
    if args.max_urls <= 0:
        errors.append("‚ùå max-urls deve ser maior que 0")
    
    if args.max_depth <= 0:
        errors.append("‚ùå max-depth deve ser maior que 0")
    
    if args.threads <= 0 or args.threads > 50:
        errors.append("‚ùå threads deve estar entre 1 e 50")
    
    if args.batch_size <= 0:
        errors.append("‚ùå batch-size deve ser maior que 0")
    
    if args.timeout <= 0:
        errors.append("‚ùå timeout deve ser maior que 0")
    
    return errors


def create_config_from_args(args):
    """üîß Cria configura√ß√£o baseada nos argumentos"""
    config = get_config()
    
    # Atualiza configura√ß√µes do crawler
    config['crawler'].update({
        'max_urls': args.max_urls,
        'max_depth': args.max_depth,
        'max_threads': args.threads,
        'timeout': args.timeout
    })
    
    # Configura√ß√µes do analisador
    config['analyzer'] = {
        'type': args.analyzer,
        'batch_size': args.batch_size
    }
    
    # Configura√ß√µes de sa√≠da
    config['output'].update({
        'folder': args.output,
        'use_emoji_names': args.emoji_names,
        'filename_prefix': args.filename
    })
    
    # Configura√ß√µes de crawler espec√≠ficas
    if args.crawler == 'smart':
        config['priority_patterns'] = [
            '/produto/', '/product/', '/categoria/', '/category/',
            '/servico/', '/service/', '/sobre/', '/about/'
        ]
    elif args.crawler == 'batch':
        config['batch_size'] = args.batch_size
    
    return config


def print_startup_info(args, config):
    """üì¢ Exibe informa√ß√µes de inicializa√ß√£o"""
    domain = urlparse(args.url).netloc
    
    print("=" * 80)
    print("üè∑Ô∏è  SEO ANALYZER ULTRA CORRIGIDO - VERS√ÉO MODULARIZADA")
    print("=" * 80)
    
    print(MSG_CORRECTIONS_IMPLEMENTED)
    print(MSG_IMPROVEMENTS)
    print(MSG_NEW_CONSOLIDATED_TAB)
    
    print("\nüìä CONFIGURA√á√ïES:")
    print(f"   üéØ URL inicial: {args.url}")
    print(f"   üìà Max URLs: {args.max_urls}")
    print(f"   üìè Max profundidade: {args.max_depth}")
    print(f"   ‚ö° Threads: {args.threads}")
    print(f"   üï∑Ô∏è Tipo de crawler: {args.crawler}")
    print(f"   üè∑Ô∏è Tipo de analisador: {args.analyzer}")
    print(f"   üì¶ Batch size: {args.batch_size}")
    print(f"   üìÅ Pasta de sa√≠da: {args.output}")
    print(f"   ‚è±Ô∏è Timeout: {args.timeout}s")
    
    print(f"\nüöÄ Iniciando an√°lise do dom√≠nio: {domain}")
    print("=" * 80)


def main():
    """üöÄ Fun√ß√£o principal"""
    try:
        # 1. Parse e valida√ß√£o de argumentos
        args = parse_arguments()
        
        validation_errors = validate_arguments(args)
        if validation_errors:
            print("‚ùå ERROS DE VALIDA√á√ÉO:")
            for error in validation_errors:
                print(f"   {error}")
            sys.exit(1)
        
        # 2. Cria configura√ß√£o
        config = create_config_from_args(args)
        
        # 3. Exibe informa√ß√µes de inicializa√ß√£o
        print_startup_info(args, config)
        
        # 4. Cria componentes modulares
        print("üîß Inicializando componentes...")
        
        crawler = create_crawler(args.crawler, config)
        print(f"   ‚úÖ Crawler '{args.crawler}' criado")
        
        analyzer = create_metatags_analyzer(args.analyzer, config)
        print(f"   ‚úÖ Analisador '{args.analyzer}' criado")
        
        report_generator = create_report_generator('default', config['output'])
        print(f"   ‚úÖ Gerador de relat√≥rios criado")
        
        # 5. Executa crawling e an√°lise
        print("\nüï∑Ô∏è FASE 1: CRAWLING E AN√ÅLISE")
        print(MSG_CRAWLER_START.format(domain=urlparse(args.url).netloc))
        
        results = crawler.crawl(
            start_url=args.url,
            max_urls=args.max_urls,
            analyzers=[analyzer]
        )
        
        if not results:
            print("‚ùå Nenhum resultado obtido do crawling!")
            sys.exit(1)
        
        print(MSG_ANALYSIS_COMPLETE.format(total_urls=len(results)))
        
        # 6. Gera relat√≥rio
        print("\nüìä FASE 2: GERA√á√ÉO DE RELAT√ìRIOS")
        
        filepath, df_principal = report_generator.generate_complete_report(
            results=results,
            crawlers_data=analyzer,
            filename_prefix=args.filename
        )
        
        if not filepath:
            print("‚ùå Erro na gera√ß√£o do relat√≥rio!")
            sys.exit(1)
        
        # 7. Exibe estat√≠sticas finais
        print("\nüìà FASE 3: ESTAT√çSTICAS FINAIS")
        print("=" * 80)
        
        # Estat√≠sticas do crawler
        crawler_stats = crawler.get_stats()
        print("üï∑Ô∏è ESTAT√çSTICAS DO CRAWLER:")
        print(f"   URLs encontradas: {crawler_stats['crawling']['urls_found']}")
        print(f"   URLs processadas: {crawler_stats['crawling']['urls_processed']}")
        print(f"   Taxa de sucesso: {crawler_stats['summary']['success_rate']:.1f}%")
        print(f"   Tempo total: {crawler_stats['summary']['total_crawling_time']:.2f}s")
        print(f"   URLs/segundo: {crawler_stats['summary']['urls_per_second']:.2f}")
        
        # Estat√≠sticas do analisador
        analyzer_stats = analyzer.get_stats()
        print(f"\nüè∑Ô∏è ESTAT√çSTICAS DO ANALISADOR:")
        print(f"   URLs analisadas: {analyzer_stats['processing']['urls_processadas']}")
        print(f"   Titles duplicados √∫nicos: {analyzer_stats['duplicates']['total_duplicate_titles']}")
        print(f"   Descriptions duplicadas √∫nicas: {analyzer_stats['duplicates']['total_duplicate_descriptions']}")
        
        # Estat√≠sticas do relat√≥rio
        if not df_principal.empty:
            print(f"\nüìä ESTAT√çSTICAS DO RELAT√ìRIO:")
            print(f"   P√°ginas no relat√≥rio: {len(df_principal)}")
            
            # Score m√©dio
            score_medio = df_principal['Metatags_Score'].mean()
            print(f"   Score m√©dio: {score_medio:.1f}/100")
            
            # Problemas cr√≠ticos
            criticos = len(df_principal[df_principal['Critical_Issues'] != ''])
            print(f"   URLs com problemas cr√≠ticos: {criticos}")
            
            # Headings problem√°ticos (NOVA M√âTRICA)
            headings_problematicos = len(df_principal[df_principal['Headings_Problematicos_Total'] > 0])
            print(f"   üÜï URLs com headings problem√°ticos: {headings_problematicos}")
            
            # Top 3 problemas
            if criticos > 0:
                top_issues = []
                
                # Title ausente
                title_ausente = len(df_principal[df_principal['Title_Status'] == 'Ausente'])
                if title_ausente > 0:
                    top_issues.append(f"Titles ausentes ({title_ausente})")
                
                # Description ausente
                desc_ausente = len(df_principal[df_principal['Description_Status'] == 'Ausente'])
                if desc_ausente > 0:
                    top_issues.append(f"Descriptions ausentes ({desc_ausente})")
                
                # H1 ausente
                h1_ausente = len(df_principal[df_principal['H1_Ausente'] == 'SIM'])
                if h1_ausente > 0:
                    top_issues.append(f"H1s ausentes ({h1_ausente})")
                
                if top_issues:
                    print(f"   Top problemas: {', '.join(top_issues[:3])}")
        
        # 8. Informa√ß√µes finais
        print("\nüéØ AN√ÅLISE CONCLU√çDA COM SUCESSO!")
        print("=" * 80)
        print(f"üìÅ Relat√≥rio gerado: {filepath}")
        print("\nüí° PRINCIPAIS CORRE√á√ïES IMPLEMENTADAS:")
        print("   ‚úÖ Headings vazios/ocultos IGNORADOS na an√°lise hier√°rquica")
        print("   ‚úÖ Detec√ß√£o EXPANDIDA de headings ocultos (cores invis√≠veis)")
        print("   ‚úÖ Consolida√ß√£o em aba √∫nica 'Headings_Problematicos'")
        print("   ‚úÖ Gravidade diferenciada (H1s = CR√çTICO, outros = M√âDIO)")
        print("   ‚úÖ Sequ√™ncias separadas (Completa vs. V√°lida)")
        
        print(f"\nüî• Abra o arquivo Excel para ver todas as 8 abas com as corre√ß√µes!")
        print("=" * 80)
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è An√°lise interrompida pelo usu√°rio")
        sys.exit(0)
        
    except Exception as e:
        print(f"\n‚ùå ERRO INESPERADO: {str(e)}")
        if hasattr(args, 'verbose') and args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


def quick_analysis(url, max_urls=50):
    """‚ö° Fun√ß√£o para an√°lise r√°pida (uso program√°tico)"""
    config = get_config()
    config['crawler']['max_urls'] = max_urls
    config['crawler']['max_threads'] = 10
    
    # Componentes
    crawler = create_crawler('default', config)
    analyzer = create_metatags_analyzer('default', config)
    report_generator = create_report_generator('default', config['output'])
    
    # Execu√ß√£o
    results = crawler.crawl(url, max_urls, [analyzer])
    
    if results:
        filepath, df = report_generator.generate_complete_report(
            results, analyzer, "QUICK_ANALYSIS"
        )
        return filepath, df, analyzer.get_stats()
    
    return None, None, None


if __name__ == "__main__":
    main()